{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2cd5c88",
   "metadata": {
    "id": "moved-collapse",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Monodepth Estimation with OpenVINO\n",
    "\n",
    "This tutorial demonstrates Monocular Depth Estimation with MidasNet in OpenVINO. Model information can be found [here](https://docs.openvino.ai/2024/omz_models_model_midasnet.html).\n",
    "\n",
    "![monodepth](https://user-images.githubusercontent.com/36741649/127173017-a0bbcf75-db24-4d2c-81b9-616e04ab7cd9.gif)\n",
    "\n",
    "### What is Monodepth?\n",
    "Monocular Depth Estimation is the task of estimating scene depth using a single image. It has many potential applications in robotics, 3D reconstruction, medical imaging and autonomous systems. This tutorial uses a neural network model called [MiDaS](https://github.com/intel-isl/MiDaS), which was developed by the [Embodied AI Foundation](https://www.embodiedaifoundation.org/). See the research paper below to learn more.\n",
    "\n",
    "R. Ranftl, K. Lasinger, D. Hafner, K. Schindler and V. Koltun, [\"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer,\"](https://ieeexplore.ieee.org/document/9178977) in IEEE Transactions on Pattern Analysis and Machine Intelligence, doi: `10.1109/TPAMI.2020.3019967`.\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Preparation](#Preparation)\n",
    "    - [Install requirements](#Install-requirements)\n",
    "    - [Imports](#Imports)\n",
    "    - [Download the model](#Download-the-model)\n",
    "- [Functions](#Functions)\n",
    "- [Select inference device](#Select-inference-device)\n",
    "- [Load the Model](#Load-the-Model)\n",
    "- [Monodepth on Image](#Monodepth-on-Image)\n",
    "    - [Load, resize and reshape input image](#Load,-resize-and-reshape-input-image)\n",
    "    - [Do inference on the image](#Do-inference-on-the-image)\n",
    "    - [Display monodepth image](#Display-monodepth-image)\n",
    "- [Monodepth on Video](#Monodepth-on-Video)\n",
    "    - [Video Settings](#Video-Settings)\n",
    "    - [Load the Video](#Load-the-Video)\n",
    "    - [Do Inference on a Video and Create Monodepth Video](#Do-Inference-on-a-Video-and-Create-Monodepth-Video)\n",
    "    - [Display Monodepth Video](#Display-Monodepth-Video)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23fa9f61",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparation\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "### Install requirements\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79589b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21503"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "%pip install -q \"openvino>=2023.1.0\"\n",
    "%pip install -q opencv-python requests tqdm\n",
    "\n",
    "if platform.system() != \"Windows\":\n",
    "    %pip install -q \"matplotlib==3.5.2\"\n",
    "else:\n",
    "    %pip install -q \"matplotlib==3.5.2\"\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ea428fa",
   "metadata": {},
   "source": [
    "### Imports\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d08d89",
   "metadata": {
    "id": "ahead-spider"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import (\n",
    "    HTML,\n",
    "    FileLink,\n",
    "    Pretty,\n",
    "    ProgressBar,\n",
    "    Video,\n",
    "    clear_output,\n",
    "    display,\n",
    ")\n",
    "import openvino as ov\n",
    "\n",
    "from notebook_utils import download_file, load_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57a29fe4",
   "metadata": {
    "id": "contained-office"
   },
   "source": [
    "### Download the model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The model is in the [OpenVINO Intermediate Representation (IR)](https://docs.openvino.ai/2024/documentation/openvino-ir-format.html) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94499763",
   "metadata": {
    "id": "amber-lithuania"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'model/MiDaS_small.xml' already exists.\n",
      "'model/MiDaS_small.bin' already exists.\n"
     ]
    }
   ],
   "source": [
    "model_folder = Path(\"model\")\n",
    "\n",
    "ir_model_url = \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/models/depth-estimation-midas/FP32/\"\n",
    "ir_model_name_xml = \"MiDaS_small.xml\"\n",
    "ir_model_name_bin = \"MiDaS_small.bin\"\n",
    "\n",
    "download_file(ir_model_url + ir_model_name_xml, filename=ir_model_name_xml, directory=model_folder)\n",
    "download_file(ir_model_url + ir_model_name_bin, filename=ir_model_name_bin, directory=model_folder)\n",
    "\n",
    "model_xml_path = model_folder / ir_model_name_xml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46a16ede",
   "metadata": {},
   "source": [
    "## Functions\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c9f693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_minmax(data):\n",
    "    \"\"\"Normalizes the values in `data` between 0 and 1\"\"\"\n",
    "    return (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "def convert_result_to_image(result, colormap=\"viridis\"):\n",
    "    \"\"\"\n",
    "    Convert network result of floating point numbers to an RGB image with\n",
    "    integer values from 0-255 by applying a colormap.\n",
    "\n",
    "    `result` is expected to be a single network result in 1,H,W shape\n",
    "    `colormap` is a matplotlib colormap.\n",
    "    See https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "    \"\"\"\n",
    "    cmap = plt.cm.get_cmap(colormap)\n",
    "    result = result.squeeze(0)\n",
    "    result = normalize_minmax(result)\n",
    "    result = cmap(result)[:, :, :3] * 255\n",
    "    result = result.astype(np.uint8)\n",
    "    return result\n",
    "\n",
    "\n",
    "def to_rgb(image_data) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert image_data from BGR to RGB\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d932f95-b9e2-4be0-a8d6-bcc54fc52e6d",
   "metadata": {},
   "source": [
    "## Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa9f2c0-1ffa-436b-b92f-657a8727a2d8",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11c2ee5a1174f88bf0d77be413f8917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU', 'CPU'), value='CPU')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"CPU\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7799cbb7",
   "metadata": {
    "id": "sensitive-wagner"
   },
   "source": [
    "## Load the Model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Load the model in OpenVINO Runtime with `core.read_model` and compile it for the specified device with `core.compile_model`. Get input and output keys and the expected input shape for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9e708e",
   "metadata": {
    "id": "complete-brother"
   },
   "outputs": [],
   "source": [
    "# Create cache folder\n",
    "cache_folder = Path(\"cache\")\n",
    "cache_folder.mkdir(exist_ok=True)\n",
    "\n",
    "core = ov.Core()\n",
    "core.set_property({\"CACHE_DIR\": cache_folder})\n",
    "model = core.read_model(model_xml_path)\n",
    "compiled_model = core.compile_model(model=model, device_name=device.value)\n",
    "\n",
    "input_key = compiled_model.input(0)\n",
    "output_key = compiled_model.output(0)\n",
    "\n",
    "network_input_shape = list(input_key.shape)\n",
    "network_image_height, network_image_width = network_input_shape[2:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe5a7171",
   "metadata": {
    "id": "compact-bargain"
   },
   "source": [
    "## Monodepth on Image\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "### Load, resize and reshape input image\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The input image is read with OpenCV, resized to network input size, and reshaped to (N,C,H,W) (N=number of images,  C=number of channels, H=height, W=width). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c4ec2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "central-psychology",
    "outputId": "d864ee96-3fbd-488d-da1a-88e730f34aad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_FILE = \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/image/coco_bike.jpg\"\n",
    "image = load_image(path=IMAGE_FILE)\n",
    "\n",
    "# Resize to input shape for network.\n",
    "resized_image = cv2.resize(src=image, dsize=(network_image_height, network_image_width))\n",
    "\n",
    "# Reshape the image to network input shape NCHW.\n",
    "input_image = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de672364",
   "metadata": {
    "id": "taken-spanking"
   },
   "source": [
    "### Do inference on the image\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Do inference, convert the result to an image, and resize it to the original image shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c048d5c",
   "metadata": {
    "id": "banner-kruger"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.cm' has no attribute 'get_cmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m compiled_model([input_image])[output_key]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Convert the network result of disparity map to an image that shows\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# distance as colors.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m result_image \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_result_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Resize back to original image shape. The `cv2.resize` function expects shape\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# in (width, height), [::-1] reverses the (height, width) shape to match this.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m result_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(result_image, image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mconvert_result_to_image\u001b[0;34m(result, colormap)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_result_to_image\u001b[39m(result, colormap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    Convert network result of floating point numbers to an RGB image with\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    integer values from 0-255 by applying a colormap.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    See https://matplotlib.org/stable/tutorials/colors/colormaps.html\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     cmap \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cmap\u001b[49m(colormap)\n\u001b[1;32m     17\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m     result \u001b[38;5;241m=\u001b[39m normalize_minmax(result)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.cm' has no attribute 'get_cmap'"
     ]
    }
   ],
   "source": [
    "result = compiled_model([input_image])[output_key]\n",
    "\n",
    "# Convert the network result of disparity map to an image that shows\n",
    "# distance as colors.\n",
    "result_image = convert_result_to_image(result=result)\n",
    "\n",
    "# Resize back to original image shape. The `cv2.resize` function expects shape\n",
    "# in (width, height), [::-1] reverses the (height, width) shape to match this.\n",
    "result_image = cv2.resize(result_image, image.shape[:2][::-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4989349b",
   "metadata": {},
   "source": [
    "### Display monodepth image\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818fe7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "ranging-executive",
    "outputId": "30373e8e-34e9-4820-e32d-764aa99d4b25"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 15))\n",
    "ax[0].imshow(to_rgb(image))\n",
    "ax[1].imshow(result_image);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63bd66b3",
   "metadata": {
    "id": "descending-cache"
   },
   "source": [
    "## Monodepth on Video\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "By default, only the first 4 seconds are processed in order to quickly check that everything works. Change `NUM_SECONDS` in the cell below to modify this. Set `NUM_SECONDS` to 0 to process the whole video."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb1cd959",
   "metadata": {},
   "source": [
    "### Video Settings\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f8b75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "terminal-dividend",
    "outputId": "87f5ada0-8caf-49c3-fe54-626e2b1967f3"
   },
   "outputs": [],
   "source": [
    "# Video source: https://www.youtube.com/watch?v=fu1xcQdJRws (Public Domain)\n",
    "VIDEO_FILE = \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/video/Coco%20Walking%20in%20Berkeley.mp4\"\n",
    "# Number of seconds of input video to process. Set `NUM_SECONDS` to 0 to process\n",
    "# the full video.\n",
    "NUM_SECONDS = 4\n",
    "# Set `ADVANCE_FRAMES` to 1 to process every frame from the input video\n",
    "# Set `ADVANCE_FRAMES` to 2 to process every second frame. This reduces\n",
    "# the time it takes to process the video.\n",
    "ADVANCE_FRAMES = 2\n",
    "# Set `SCALE_OUTPUT` to reduce the size of the result video\n",
    "# If `SCALE_OUTPUT` is 0.5, the width and height of the result video\n",
    "# will be half the width and height of the input video.\n",
    "SCALE_OUTPUT = 0.5\n",
    "# The format to use for video encoding. The 'vp09` is slow,\n",
    "# but it works on most systems.\n",
    "# Try the `THEO` encoding if you have FFMPEG installed.\n",
    "# FOURCC = cv2.VideoWriter_fourcc(*\"THEO\")\n",
    "FOURCC = cv2.VideoWriter_fourcc(*\"vp09\")\n",
    "\n",
    "# Create Path objects for the input video and the result video.\n",
    "output_directory = Path(\"output\")\n",
    "output_directory.mkdir(exist_ok=True)\n",
    "result_video_path = output_directory / f\"{Path(VIDEO_FILE).stem}_monodepth.mp4\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "babf160c",
   "metadata": {},
   "source": [
    "### Load the Video\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Load the video from a `VIDEO_FILE`, set in the *Video Settings* cell above. Open the video to read the frame width and height and fps, and compute values for these properties for the monodepth video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a80192",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(str(VIDEO_FILE))\n",
    "ret, image = cap.read()\n",
    "if not ret:\n",
    "    raise ValueError(f\"The video at {VIDEO_FILE} cannot be read.\")\n",
    "input_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "input_video_frame_height, input_video_frame_width = image.shape[:2]\n",
    "\n",
    "target_fps = input_fps / ADVANCE_FRAMES\n",
    "target_frame_height = int(input_video_frame_height * SCALE_OUTPUT)\n",
    "target_frame_width = int(input_video_frame_width * SCALE_OUTPUT)\n",
    "\n",
    "cap.release()\n",
    "print(f\"The input video has a frame width of {input_video_frame_width}, \" f\"frame height of {input_video_frame_height} and runs at {input_fps:.2f} fps\")\n",
    "print(\n",
    "    \"The monodepth video will be scaled with a factor \"\n",
    "    f\"{SCALE_OUTPUT}, have width {target_frame_width}, \"\n",
    "    f\" height {target_frame_height}, and run at {target_fps:.2f} fps\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "425ef846",
   "metadata": {},
   "source": [
    "### Do Inference on a Video and Create Monodepth Video\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e62ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "present-albany",
    "outputId": "600edb69-af12-44dc-ec8e-95005b74179c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize variables.\n",
    "input_video_frame_nr = 0\n",
    "start_time = time.perf_counter()\n",
    "total_inference_duration = 0\n",
    "\n",
    "# Open the input video\n",
    "cap = cv2.VideoCapture(str(VIDEO_FILE))\n",
    "\n",
    "# Create a result video.\n",
    "out_video = cv2.VideoWriter(\n",
    "    str(result_video_path),\n",
    "    FOURCC,\n",
    "    target_fps,\n",
    "    (target_frame_width * 2, target_frame_height),\n",
    ")\n",
    "\n",
    "num_frames = int(NUM_SECONDS * input_fps)\n",
    "total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT) if num_frames == 0 else num_frames\n",
    "progress_bar = ProgressBar(total=total_frames)\n",
    "progress_bar.display()\n",
    "\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "        if not ret:\n",
    "            cap.release()\n",
    "            break\n",
    "\n",
    "        if input_video_frame_nr >= total_frames:\n",
    "            break\n",
    "\n",
    "        # Only process every second frame.\n",
    "        # Prepare a frame for inference.\n",
    "        # Resize to the input shape for network.\n",
    "        resized_image = cv2.resize(src=image, dsize=(network_image_height, network_image_width))\n",
    "        # Reshape the image to network input shape NCHW.\n",
    "        input_image = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), 0)\n",
    "\n",
    "        # Do inference.\n",
    "        inference_start_time = time.perf_counter()\n",
    "        result = compiled_model([input_image])[output_key]\n",
    "        inference_stop_time = time.perf_counter()\n",
    "        inference_duration = inference_stop_time - inference_start_time\n",
    "        total_inference_duration += inference_duration\n",
    "\n",
    "        if input_video_frame_nr % (10 * ADVANCE_FRAMES) == 0:\n",
    "            clear_output(wait=True)\n",
    "            progress_bar.display()\n",
    "            # input_video_frame_nr // ADVANCE_FRAMES gives the number of\n",
    "            # Frames that have been processed by the network.\n",
    "            display(\n",
    "                Pretty(\n",
    "                    f\"Processed frame {input_video_frame_nr // ADVANCE_FRAMES}\"\n",
    "                    f\"/{total_frames // ADVANCE_FRAMES}. \"\n",
    "                    f\"Inference time per frame: {inference_duration:.2f} seconds \"\n",
    "                    f\"({1/inference_duration:.2f} FPS)\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Transform the network result to a RGB image.\n",
    "        result_frame = to_rgb(convert_result_to_image(result))\n",
    "        # Resize the image and the result to a target frame shape.\n",
    "        result_frame = cv2.resize(result_frame, (target_frame_width, target_frame_height))\n",
    "        image = cv2.resize(image, (target_frame_width, target_frame_height))\n",
    "        # Put the image and the result side by side.\n",
    "        stacked_frame = np.hstack((image, result_frame))\n",
    "        # Save a frame to the video.\n",
    "        out_video.write(stacked_frame)\n",
    "\n",
    "        input_video_frame_nr = input_video_frame_nr + ADVANCE_FRAMES\n",
    "        cap.set(1, input_video_frame_nr)\n",
    "\n",
    "        progress_bar.progress = input_video_frame_nr\n",
    "        progress_bar.update()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Processing interrupted.\")\n",
    "finally:\n",
    "    clear_output()\n",
    "    processed_frames = num_frames // ADVANCE_FRAMES\n",
    "    out_video.release()\n",
    "    cap.release()\n",
    "    end_time = time.perf_counter()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    print(\n",
    "        f\"Processed {processed_frames} frames in {duration:.2f} seconds. \"\n",
    "        f\"Total FPS (including video processing): {processed_frames/duration:.2f}.\"\n",
    "        f\"Inference FPS: {processed_frames/total_inference_duration:.2f} \"\n",
    "    )\n",
    "    print(f\"Monodepth Video saved to '{str(result_video_path)}'.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe7e5c1d",
   "metadata": {
    "id": "bZ89ZI369KjA"
   },
   "source": [
    "### Display Monodepth Video\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233d3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video = Video(result_video_path, width=800, embed=True)\n",
    "if not result_video_path.exists():\n",
    "    plt.imshow(stacked_frame)\n",
    "    raise ValueError(\"OpenCV was unable to write the video file. Showing one video frame.\")\n",
    "else:\n",
    "    print(f\"Showing monodepth video saved at\\n{result_video_path.resolve()}\")\n",
    "    print(\"If you cannot see the video in your browser, please click on the \" \"following link to download the video \")\n",
    "    video_link = FileLink(result_video_path)\n",
    "    video_link.html_link_str = \"<a href='%s' download>%s</a>\"\n",
    "    display(HTML(video_link._repr_html_()))\n",
    "    display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e80ae-b778-49af-9349-69bd73188b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/15709723/127752390-f6aa371f-31b5-4846-84b9-18dd4f662406.gif",
   "tags": {
    "categories": [
     "Model Demos"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Depth Estimation"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
