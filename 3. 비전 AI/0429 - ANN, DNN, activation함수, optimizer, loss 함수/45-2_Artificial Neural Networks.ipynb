{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공 신경망(Artificial Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 정보를 어떻게 처리하고 학습하는지 궁금해 본 적이 있습니까? 예를 들어, 우리 몸은 손이나 다리를 움직일 수 있도록 정보를 어떻게 처리할까요? 간단하게 말하면 뇌에서 정보를 처리한 다음 몸의 다른 부분으로 신호를 보내 특정 근육의 움직임을 유도합니다. 이 신호는 신경계를 통해 전달됩니다. 신경계의 주요 구성 요소 중 하나는 뉴런 세포입니다. 이 세포들은 신호가 특정 값 또는 양보다 높은 경우에만 다른 세포로 신호를 전송합니다. 즉, 임계값을 기준으로 작동합니다. 따라서 우리가 손을 움직이기로 결정하면, 뇌의 신호는 다리 근육이 아닌 손 근육으로 뉴런 세포들을 통해 신호를 전달합니다.\n",
    "\n",
    "### 인공신경망 훈련\n",
    "\n",
    "그러나 이것은 우리가 정보를 처리하는 방법에 대해서만 설명해 줍니다. 인간의 학습 능력은 어떻게 작동하나요? 예를 들어, 우리는 빨간불에 멈춰서야 한다는 것을 알고 있고, 공을 차는 방법을 알고 있습니다. 다른 사람들이 하는 방법이나 사례를 보고 그렇게 하도록 훈련되었기 때문입니다. 이러한 예들을 통해 우리는 배우고 기억할 수 있었습니다.\n",
    "\n",
    "컴퓨터가 인간이 정보를 처리하고 학습하는 방식을 모방할 수 있다면 얼마나 좋을까요? 인공 신경망으로 이것이 가능합니다! 인공 신경망은 데이터 세트 내에서 복잡한 관계를 처리하고 '학습'할 수 있습니다. 간단한 신경망의 개념도는 아래와 같습니다.\n",
    "\n",
    "기본 아이디어는 입력 레이어(input layer)에 데이터를 입력하고, 뒤의 은닉 레이어(hidden layer)에서 데이터를 처리한다는 것입니다. 아래 그림에서 은닉 레이어가 하나만 표시되어 있지만 여러개의 은닉 레이어로 구성될 수  있습니다. 각 레이어는 데이터에 기능을 적용하고 다른 은닉 레이어로 전달한 후 최종적으로 출력 레이어(Output layer)로 끝나는 여러 개의 인공 뉴런으로 구성됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"resources/ANN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림은 1개의 입력 레이어, 1개의 은닉 레이어(입력 레이어와 출력 레이어 사이) 및 1개의 출력 레이어로 구성된 간단한 신경망을 보여줍니다. 각 원은 1개의 노드 또는 1개의 뉴런을 나타냅니다. 입력 레이어는 모델에 전달되는 데이터일 뿐이므로 일반적으로 모델 아키텍처에서 입력 레이어의 노드 수는 이야기하지 않습니다. 위의 신경망 모델의 은닉 레이어는 2개의 노드/뉴런이 있고 출력 레이어에는 1개의 노드/뉴런이 있습니다.\n",
    "\n",
    "출력 레이어는 신경망의 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망은 어떻게 작동합니까? 기계 학습 프로젝트에 어떻게 유용할 수 있습니까? 인공 신경망에 대해 자세히 알아보려면 이 [동영상](https://www.youtube.com/watch?v=aircAruvnKk) 을 시청하세요. 영상을 일시 중지하고 신경망이 어떻게 작동하는지 이해하는 시간을 가져보세요. 워크시트에 신경망에 대해 흥미로운 정보를 기록해 두십시오. 5개의 노드가 가진 1개의 입력 레이어, 각각 3개의 노드가 있는 2개의 은닉 레이어, 2개의 노드가 있는 1개의 출력 레이어로 구성된 네트워크 그려보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인공 신경망의 다양한 기능을 이해한 후에도 여전히 남아 있는 한 가지 의문점은 네트워크가 \"학습\"하는 방법입니다.\n",
    "\n",
    "3점 슛을 배우는 젊은 농구 선수를 생각해 봅시다. 슛을 하고 슛이 너무 짧아서 실패하면 농구 선수는 다음 슛의 강도를 높여서 거리감을 조정합니다. 다음 슛이 골대에서 너무 오른쪽으로 간다면 선수는 다음 슛이 골대 중앙을 향해 갈 수 있도록 방향을 조정합니다. 선수는 슛이 성공할 때까지 이 동작을 계속합니다. 이후 선수는 3점 슛을 쏠 때 성공했던 상황의 힘과 방향을 기억합니다.\n",
    "\n",
    "이것은 신경망이 훈련되는 방식과 유사합니다. 먼저 데이터가 네트워크를 통해 전달되고 예측된 출력이 제공됩니다. 이것을 순전파라고 합니다. 그런 다음 예측된 출력은 데이터의 실제 출력과 비교되고, 예측 출력과 실제 출력의 차이는 모델을 통해 뒤로 전달됩니다. 역방향으로 전파 동안 예측 출력과 실제 출력 간의 차이가 줄어들도록 모델 내에서 조정이 이루어집니다. 이것을 역전파라고 합니다. 조정이 이루어진 후, 데이터는 입력 레이어에서 다시 전달되고 또 순전파를 통해 다른 예측된 출력이 만들어집니다. 새로운 예측 출력은 실제 출력과 다시 비교되고 그 차이는 모델을 통해 역방향으로 다시 전달됩니다. 모델 내에서 더 많은 조정이 이루어지게 됩니다.\n",
    "\n",
    "예측 출력과 실제 출력의 차이가 최소화될 때까지 순전파와 역전파의 과정을 반복합니다. 최종적으로 모델이 학습되어 다른 유사한 데이터 세트의 예측에 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망이 훈련되는 방식과 비슷한 생활 속의 또 다른 예를 찾아볼 수 있습니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망을 조금 더 이해할 수 있도록 다른 예를 살펴봅시다. 여러분은 아버지의 생일을 맞이하여 생일 케이크를 만드는 임무를 부여받았습니다. 요리책을 보고 기본적인 케이크를 만들 수 있습니다. 여러분은 지금까지 먹어본 케이크 중 가장 맛있는 케이크가 되길 바라는 마음에 엄마에게 시식을 부탁하러 갑니다. 여러분의 어머니는 케이크가 너무 달고 약간 탄거 같다고 대답합니다. 여러분은 다시 케이크를 만들면서 설탕의 양과 오븐에서 굽는 시간을 조정하려고 노력합니다. 새로운 레시피와 베이킹 시간으로 또 다른 케이크를 만든 다음에 어머님께 다시 맛 테스트를 요청할 것입니다. 이것은 여러분이 완벽한 케이크를 만들 때까지 계속될 것입니다.\n",
    "\n",
    "케이크를 굽는 첫 번째 단계는 인공 신경망 내에서 순전파 단계와 유사합니다. 어머니의 맛 테스트는 예상 출력과 실제 출력을 비교하는 것과 유사합니다. 설탕의 양과 베이킹 시간을 조정하는 것은 신경망 내에서 역전파와 유사합니다. 단계의 반복은 모델의 전체 학습 프로세스와 유사합니다.\n",
    "\n",
    "역전파의 작동 방식을 시각적으로 잘 이해하려면 이 [동영상](https://www.youtube.com/watch?v=Ilg3gGewQ5U) 을 보고 관심 있는 정보를 모두 기록해 두십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>보너스: 모델 내에서 조정되는 방법을 이해할 필요는 없습니다. 그러나 수학적으로 관심이 있거나 모든 조정을 이해하는 데 정말로 관심이 있고 시간이 있다면 아래 나열된 2개의 동영상을 시청해 보세요. 워크시트나 아래 셀에 관심있는 정보를 기록해 두십시오. </font>\n",
    "- [동영상 1](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "- [동영상 2](https://www.youtube.com/watch?v=tIeHLnjs5U8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Flower 데이터 세트를 이용한 신경망 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris Flower 데이터 세트를 사용하여 신경망을 훈련시켜 봅시다!\n",
    "\n",
    "## 1. 필수 라이브러리 가져오기\n",
    "먼저 필요한 라이브러리를 가져옵니다. pandas 및 numpy는 데이터 구조를 제공하고 scikit은 인공 신경망에 액세스하는 방법을 배울 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./resources/PetalSepal1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 세트 획득 및 탐색\n",
    "\n",
    "Iris Flower 데이터 세트를 아래의 데이터 프레임 df로 가져옵니다.\n",
    "파일은 iris.data 입니다.\n",
    "\n",
    "모든 열의 헤더를 포함하는 것을 잊지 마십시오.\n",
    "\n",
    "변수를 이해하려면 위의 그림(출처: https://www.researchgate.net/Figure/Trollius-ranunculoide-flower-with-measured-traits_fig6_272514310) 을 참조하십시오.\n",
    "\n",
    "먼저 데이터를 탐색하겠습니다. 데이터 탐색이 어떻게 진행되었는지 기업하십니까?\n",
    "\n",
    "1. csv 파일을 열고 데이터 프레임에 넣습니다.\n",
    "2. 헤더를 포함합니다\n",
    "3. .info() 및 .describe()를 사용하여 데이터 세트에 대한 기본 정보를 확인합니다.\n",
    "\n",
    "누락된 값이나 오류 데이터가 있는지 확인합니다. 누락된 데이터가 있습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width        class\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   class         150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "None\n",
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "count    150.000000   150.000000    150.000000   150.000000\n",
      "mean       5.843333     3.054000      3.758667     1.198667\n",
      "std        0.828066     0.433594      1.764420     0.763161\n",
      "min        4.300000     2.000000      1.000000     0.100000\n",
      "25%        5.100000     2.800000      1.600000     0.300000\n",
      "50%        5.800000     3.000000      4.350000     1.300000\n",
      "75%        6.400000     3.300000      5.100000     1.800000\n",
      "max        7.900000     4.400000      6.900000     2.500000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/iris.data\",header=None)\n",
    "names = [\"sepal_length\", \"sepal_width\",\"petal_length\", \"petal_width\", \"class\"]\n",
    "df.columns = names\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 특성 및 목표 값 결정\n",
    "\n",
    "이제 데이터 세트를 x 값(모델이 관계를 학습할 수 있는 특성)과 y 값(목표 값 또는 모델의 예상 출력)으로 분할해야 합니다.\n",
    "\n",
    "### 표준화\n",
    "데이터 세트도 표준화해야 합니다.\n",
    "표준화의 용도는 무엇입니까? 이를 이해하려면 위의 데이터 분포를 살펴보십시오! 특성 데이터들은 서로 다른 평균과 표준 편차를 가지고 있습니다.이러한 변수를 비교하는 것은 어렵습니다. 표준화는 이러한 특성 데이터 들의 평균과 표준 편차를 균등화하여 쉽게 비교할 수 있도록 도와줍니다.\n",
    "\n",
    "scikit-learn 라이브러리의 스케일링 관련 [자료](http://benalexkeen.com/feature-scaling-with-scikit-learn/) 를 참조하십시오. 스케일링 전후에 데이터가 어떻게 변경되었는지 확인하십시오.\n",
    "\n",
    "이는 신경망이 쉽게 분류 작업을 할 수 있도록 하기 위한 것입니다. 아래 코드는 x 값을 x_value로 추출하고 표준화합니다. y_values는 나중에 추출합니다. 아래 코드를 실행하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width\n",
      "0           5.1          3.5           1.4          0.2\n",
      "1           4.9          3.0           1.4          0.2\n",
      "2           4.7          3.2           1.3          0.2\n",
      "3           4.6          3.1           1.5          0.2\n",
      "4           5.0          3.6           1.4          0.2\n"
     ]
    }
   ],
   "source": [
    "x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "print(x_values.head())\n",
    "\n",
    "standardise = StandardScaler() # 표준척도는 분포의 평균값이 0이고 표준편차가 1이 되도록 데이터를 변환합니다.\n",
    "\n",
    "x_values = standardise.fit_transform(x_values)\n",
    "x_values_df = pd.DataFrame(x_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표준화된 데이터 세트를 원본 데이터세트와 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0 -0.900681  1.032057 -1.341272 -1.312977\n",
      "1 -1.143017 -0.124958 -1.341272 -1.312977\n",
      "2 -1.385353  0.337848 -1.398138 -1.312977\n",
      "3 -1.506521  0.106445 -1.284407 -1.312977\n",
      "4 -1.021849  1.263460 -1.341272 -1.312977\n"
     ]
    }
   ],
   "source": [
    "print(x_values_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".describe 함수를 사용하여 표준화된 데이터 세트의 현재 평균과 표준 값을 확인해 보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0             1             2             3\n",
      "count  1.500000e+02  1.500000e+02  1.500000e+02  1.500000e+02\n",
      "mean  -4.736952e-16 -6.631732e-16  3.315866e-16 -2.842171e-16\n",
      "std    1.003350e+00  1.003350e+00  1.003350e+00  1.003350e+00\n",
      "min   -1.870024e+00 -2.438987e+00 -1.568735e+00 -1.444450e+00\n",
      "25%   -9.006812e-01 -5.877635e-01 -1.227541e+00 -1.181504e+00\n",
      "50%   -5.250608e-02 -1.249576e-01  3.362659e-01  1.332259e-01\n",
      "75%    6.745011e-01  5.692513e-01  7.627586e-01  7.905908e-01\n",
      "max    2.492019e+00  3.114684e+00  1.786341e+00  1.710902e+00\n"
     ]
    }
   ],
   "source": [
    "print(x_values_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 신경망 구축\n",
    "\n",
    "이제 간단한 신경망을 구축해 봅시다. 그러기 위해서 keras 라이브러리에서 Dense와 Sequential 함수를 가져와야 합니다.\n",
    "\n",
    "### Sequential\n",
    "Sequential 모델을 사용하면 먼저 빈 모델 객체를 만든 다음 레이어를 순서대로 차례로 추가할 수 있습니다.\n",
    "\n",
    "### Dense\n",
    "Dense 레이어는 간단히 신경망의 뉴런 레이어라고 생각하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 레이어 1개, 은닉 레이어 2개, 출력 레이어 1개로 신경망을 구축해 보겠습니다. \n",
    "은닉 레이어 내에 몇 개의 노드가 있어야 하는지를 결정하는 규칙은 없습니다. 이 신경망의 경우 각 은닉 레이어에 6개의 노드를 사용합니다.\n",
    "\n",
    "출력 레이어는 클래스 수만큼 노드를 사용해야 합니다. iris 데이터 세트의 경우 출력 레이어에 몇 개의 노드를 사용해야 합니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3개의 node가 필요로 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 만드는 신경망을 그려보세요."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인공 신경망을 구축하기 위해 아래 코드를 실행해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IOT\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 신경망 모델을 초기화\n",
    "model = Sequential()\n",
    "\n",
    "# 6개의 노드가 있는 첫 번째 은닉 레이어를 추가합니다.\n",
    "# Input_dim은 x_values 또는 입력 레이어의 수/특성 수를 나타냅니다.\n",
    "# activation은 노드/뉴런이 활성화되는 방식을 나타냅니다. 우리는 relu를 사용할 것입니다. 다른 일반적인 활성화 방식은'sigmoid' 및 'tanh'입니다.\n",
    "model.add(Dense(6, input_dim=4, activation='relu'))\n",
    "#model.add(Dense(6, activation='relu', input_shape=(4,)))\n",
    "\n",
    "# 6개의 노드가 있는 두 번째 은닉 레이어를 추가합니다. \n",
    "model.add(Dense(6, activation='relu'))\n",
    "\n",
    "# 3개의 노드가 있는 출력 레이어를 추가합니다.\n",
    "# 사용된 activation은 'softmax'입니다. Softmax는 범주형 출력 또는 대상을 처리할 때 사용됩니다.\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# 모델을 컴파일합니다. optimizer는 모델 내에서 조정하는 방법을 의미합니다. loss은 예측된 출력과 실제 출력 간의 차이를 나타냅니다.\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선택 사항: [활성화 함수 간의 비교](http://www.machineintellegence.com/different-types-of-activation-functions-in-keras/)\n",
    "\n",
    "이전 Acquire-CV에서 ReLu에 대해 학습하였습니다. 더 많은 활성화 함수가 있습니다. 특정 데이터/입력이 뉴런을 따라갈 수 있도록 하는 [on-off 버튼](https://en.wikipedia.org/wiki/Activation_function) 과 같습니다. 지금은 이 함수에 대하여 자세히 알 필요는 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컴파일된 후에 모델의 요약된 정보를 출력할 수 있습니다. 아래 코드를 실행하여 확인하세요.\n",
    "- 모델의 레이어와 순서\n",
    "- 각 레이어의 출력 형태\n",
    "- 각 레이어의 매개변수(가중치) 수\n",
    "- 모델의 총 매개변수(가중치) 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │              \u001b[38;5;34m30\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │              \u001b[38;5;34m42\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │              \u001b[38;5;34m21\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93</span> (372.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93\u001b[0m (372.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93</span> (372.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93\u001b[0m (372.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_values는 원래 범주형 데이터이므로(숫자 대신 꽃 이름) 신경망을 훈련시키기 전에 범주의 y_value를 숫자로 변환하는 작업을해야 합니다. 신경망의 경우 범주가 숫자 그룹(예: 1,2,3,4 등)이 아닌 경우 원-핫 인코딩을 수행하기 전에 라벨 인코딩(이전 노트북을 참조하세요)을 먼저 수행해야 합니다.\n",
    "\n",
    "원-핫 인코딩에 대한 자세한 내용은 \"지도 학습 기술\" 노트북의 보너스 섹션을 참조하십시오. 이 [문서](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) 에서 더 자세히 알아볼 수도 있습니다. Keras의 to_categorical 함수를 사용하여 진행할 수 있습니다. 아래 코드를 실행하여 라벨 인코딩을 한 다음 y_values를 원-핫 인코딩하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa        50\n",
      "Iris-versicolor    50\n",
      "Iris-virginica     50\n",
      "Name: class, dtype: int64\n",
      "0    50\n",
      "1    50\n",
      "2    50\n",
      "Name: class, dtype: int64\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#from keras.utils import to_categorica\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 각 클래스의 데이터 포인트 수 출력\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# 각기 다른 클래스에 대하여 다른 숫자를 지정한 딕셔너리\n",
    "# 원-핫 인코딩으로 4개 열이 아닌 3개 열만 생성되도록 0부터 시작하는 값을 사용합니다.\n",
    "label_encode = {\"class\": {\"Iris-setosa\":0, \"Iris-versicolor\":1, \"Iris-virginica\":2}}\n",
    "\n",
    "# .replace를 사용하여 다른 클래스를 숫자로 변경\n",
    "df.replace(label_encode,inplace=True)\n",
    "\n",
    "# 각 클래스의 데이터 포인트 수를 출력하여 클래스가 숫자로 변경되었는지 확인\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# 클래스를 y_values로 추출\n",
    "y_values = df['class']\n",
    "\n",
    "# y_values 원-핫 인코딩\n",
    "y_values = to_categorical(y_values)\n",
    "\n",
    "print(y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 y_values에 나열된 내용을 확인하세요. 꽃 이름을 숫자로 인코딩했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 훈련시켜 봅시다. 아래 코드를 실행하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3866 - loss: 0.9714  \n",
      "Epoch 2/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3484 - loss: 0.9821 \n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3467 - loss: 0.9595  \n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408us/step - accuracy: 0.3660 - loss: 0.9652\n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4544 - loss: 0.9027 \n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5486 - loss: 0.8800 \n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5871 - loss: 0.8615 \n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6128 - loss: 0.8146 \n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6483 - loss: 0.8168 \n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6828 - loss: 0.7881 \n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.7142 - loss: 0.7759  \n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - accuracy: 0.6784 - loss: 0.7733\n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7677 - loss: 0.7448 \n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7916 - loss: 0.7672 \n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.8151 - loss: 0.7084  \n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8716 - loss: 0.6627 \n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8643 - loss: 0.6955 \n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8619 - loss: 0.6586 \n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8920 - loss: 0.6473 \n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118us/step - accuracy: 0.8859 - loss: 0.6368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21b5ea101f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_values와 y_values로 모델을 훈련시킵니다.\n",
    "# Epoch는 전체 데이터 세트가 모델을 학습하는 데 사용되는 횟수를 나타냅니다.\n",
    "# Shuffle = True는 모델이 각 Epoch 후에 데이터 세트의 배열을 무작위로 지정하도록 지시합니다.\n",
    "model.fit(x_values,y_values,epochs=20,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue> <b> 축하합니다! 여러분의 첫 번째 신경망을 훈련했습니다.</font> \n",
    "정확도를 살펴보기 전에 사용되는 몇가지 용어를 이해해야 합니다.\n",
    "\n",
    "- Epoch는 전체 데이터 세트가 모델을 학습하는 데 사용되는 횟수를 나타냅니다.\n",
    "\n",
    "- ㎲/step은 모델이 각 에포크에서 학습하는 데 걸린 시간을 보여줍니다.\n",
    "\n",
    "- acc는 모델이 얼마나 정확한지 보여줍니다.\n",
    "\n",
    "숫자가 각 epoch에 따라 어떻게 변경되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 모델에서 얻은 정확도 값은 얼마입니까?\n",
    "\n",
    "모델에 다른 은닉 레이어를 추가하여 더 나은 정확도를 얻을 수 있는지 확인하십시오. \n",
    "추가 은닉 레이어는 이전 레이어와 동일한 수의 노드를 가질 수 있습니다.\n",
    "\n",
    "위에 나열된 관련 코드를 아래 셀에 복사하고 코드를 수정하여 은닉 레이어를 추가합니다. 새 모델을 훈련시키고 정확도가 향상되는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IOT\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m4,128\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m330\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,131</span> (20.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,131\u001b[0m (20.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,131</span> (20.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,131\u001b[0m (20.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(128,input_dim=4,activation='relu'))\n",
    "model2.add(Dense(32,activation='relu'))\n",
    "model2.add(Dense(10,activation='relu'))\n",
    "model2.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# 모델을 컴파일합니다. optimizer는 모델 내에서 조정하는 방법을 의미합니다. loss은 예측된 출력과 실제 출력 간의 차이를 나타냅니다.\n",
    "model2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# 모델 요약 출력\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3878 - loss: 1.0579  \n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4487 - loss: 0.9948 \n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4633 - loss: 0.9342 \n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5615 - loss: 0.8634 \n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.8104 \n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - accuracy: 0.8381 - loss: 0.7481\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8211 - loss: 0.6903 \n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8563 - loss: 0.6187 \n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8438 - loss: 0.5736 \n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8383 - loss: 0.5461 \n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8753 - loss: 0.4873 \n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8448 - loss: 0.4428 \n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.3906 \n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8781 - loss: 0.3695 \n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9034 - loss: 0.3351 \n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9186 - loss: 0.2528 \n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9108 - loss: 0.2649 \n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9370 - loss: 0.2406 \n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9467 - loss: 0.2063 \n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9536 - loss: 0.1969 \n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9549 - loss: 0.1831 \n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9532 - loss: 0.1811 \n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9615 - loss: 0.1625 \n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9641 - loss: 0.1567 \n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9497 - loss: 0.1395 \n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9536 - loss: 0.1317 \n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9711 - loss: 0.1149 \n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - accuracy: 0.9628 - loss: 0.1223\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9716 - loss: 0.1012 \n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9715 - loss: 0.0930 \n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9607 - loss: 0.0976 \n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9755 - loss: 0.0827 \n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9816 - loss: 0.0771 \n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9633 - loss: 0.0832 \n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9837 - loss: 0.0723 \n",
      "Epoch 36/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9572 - loss: 0.0830 \n",
      "Epoch 37/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0745 \n",
      "Epoch 38/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0723 \n",
      "Epoch 39/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.0648 \n",
      "Epoch 40/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9798 - loss: 0.0669 \n",
      "Epoch 41/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9581 - loss: 0.0860 \n",
      "Epoch 42/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0514 \n",
      "Epoch 43/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9834 - loss: 0.0705 \n",
      "Epoch 44/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9864 - loss: 0.0536 \n",
      "Epoch 45/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9794 - loss: 0.0512 \n",
      "Epoch 46/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9664 - loss: 0.0657 \n",
      "Epoch 47/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9638 - loss: 0.0615 \n",
      "Epoch 48/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9794 - loss: 0.0602 \n",
      "Epoch 49/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9903 - loss: 0.0406 \n",
      "Epoch 50/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9811 - loss: 0.0500 \n",
      "Epoch 51/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9746 - loss: 0.0453 \n",
      "Epoch 52/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.0581 \n",
      "Epoch 53/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9877 - loss: 0.0396 \n",
      "Epoch 54/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9895 - loss: 0.0410 \n",
      "Epoch 55/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0574 \n",
      "Epoch 56/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9746 - loss: 0.0493 \n",
      "Epoch 57/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196us/step - accuracy: 0.9794 - loss: 0.0494\n",
      "Epoch 58/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9807 - loss: 0.0457 \n",
      "Epoch 59/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9903 - loss: 0.0373 \n",
      "Epoch 60/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9890 - loss: 0.0351 \n",
      "Epoch 61/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9807 - loss: 0.0369 \n",
      "Epoch 62/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9834 - loss: 0.0396 \n",
      "Epoch 63/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0418 \n",
      "Epoch 64/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.0556 \n",
      "Epoch 65/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.0493 \n",
      "Epoch 66/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - accuracy: 0.9877 - loss: 0.0336\n",
      "Epoch 67/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9873 - loss: 0.0321 \n",
      "Epoch 68/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.0597 \n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9943 - loss: 0.0332 \n",
      "Epoch 70/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.0431 \n",
      "Epoch 71/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9738 - loss: 0.0442 \n",
      "Epoch 72/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9873 - loss: 0.0328 \n",
      "Epoch 73/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9817 - loss: 0.0512  \n",
      "Epoch 74/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9790 - loss: 0.0417  \n",
      "Epoch 75/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9686 - loss: 0.0443 \n",
      "Epoch 76/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9755 - loss: 0.0440 \n",
      "Epoch 77/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9790 - loss: 0.0409 \n",
      "Epoch 78/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9716 - loss: 0.0442  \n",
      "Epoch 79/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - accuracy: 0.9925 - loss: 0.0350\n",
      "Epoch 80/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9860 - loss: 0.0293  \n",
      "Epoch 81/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9869 - loss: 0.0365  \n",
      "Epoch 82/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9890 - loss: 0.0247 \n",
      "Epoch 83/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9781 - loss: 0.0347 \n",
      "Epoch 84/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9907 - loss: 0.0224 \n",
      "Epoch 85/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9712 - loss: 0.0483 \n",
      "Epoch 86/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9842 - loss: 0.0290 \n",
      "Epoch 87/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9834 - loss: 0.0274 \n",
      "Epoch 88/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9703 - loss: 0.0472 \n",
      "Epoch 89/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9886 - loss: 0.0326 \n",
      "Epoch 90/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9686 - loss: 0.0414 \n",
      "Epoch 91/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9812 - loss: 0.0303 \n",
      "Epoch 92/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.0391 \n",
      "Epoch 93/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9834 - loss: 0.0409 \n",
      "Epoch 94/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9768 - loss: 0.0461 \n",
      "Epoch 95/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9912 - loss: 0.0282  \n",
      "Epoch 96/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9739 - loss: 0.0446  \n",
      "Epoch 97/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0241 \n",
      "Epoch 98/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0306 \n",
      "Epoch 99/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9956 - loss: 0.0191 \n",
      "Epoch 100/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9764 - loss: 0.0344 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21b5f213880>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "model2.fit(x_values,y_values,epochs=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은닉 레이어를 추가한 후 정확도가 초기 모델보다 낮아졌습니다. 따라서 더 많은 레이어를 추가하는 것이 반드시 더 높은 정확도를 보장하는 것은 아닙니다.\n",
    "\n",
    "<font color = blue>보너스: 정확도를 향상시키기 위해 은닉 레이어의 노드 숫자를 늘려볼 수 있습니다. 은닉 레이어의 노드 숫자를 늘리면 정확도가 개선이 되나요?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │             \u001b[38;5;34m195\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,091</span> (35.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,091\u001b[0m (35.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,091</span> (35.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,091\u001b[0m (35.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(128,input_dim=4,activation='relu'))\n",
    "model3.add(Dense(64,activation='relu'))\n",
    "model3.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# 모델을 컴파일합니다. optimizer는 모델 내에서 조정하는 방법을 의미합니다. loss은 예측된 출력과 실제 출력 간의 차이를 나타냅니다.\n",
    "model3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# 모델 요약 출력\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6193 - loss: 1.0109  \n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7680 - loss: 0.8526 \n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8454 - loss: 0.6910 \n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8286 - loss: 0.5994 \n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8807 - loss: 0.4836 \n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.8446 - loss: 0.4237  \n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.8142 - loss: 0.4148  \n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8712 - loss: 0.3557 \n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8617 - loss: 0.3317 \n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8774 - loss: 0.2962 \n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216us/step - accuracy: 0.8536 - loss: 0.3360\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8884 - loss: 0.2721 \n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - accuracy: 0.8994 - loss: 0.2663\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9102 - loss: 0.2274 \n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8898 - loss: 0.2458 \n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9085 - loss: 0.2087 \n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9025 - loss: 0.2237 \n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9214 - loss: 0.2010 \n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9488 - loss: 0.1993 \n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9461 - loss: 0.1721 \n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184us/step - accuracy: 0.9462 - loss: 0.1925\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9554 - loss: 0.1690 \n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9506 - loss: 0.1548 \n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9497 - loss: 0.1448 \n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9685 - loss: 0.1352 \n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9624 - loss: 0.1305 \n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9624 - loss: 0.1229 \n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9581 - loss: 0.1231 \n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - accuracy: 0.9615 - loss: 0.1125\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9585 - loss: 0.0998 \n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9663 - loss: 0.1051 \n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9737 - loss: 0.0878 \n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9819 - loss: 0.0679 \n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - accuracy: 0.9668 - loss: 0.0725\n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9607 - loss: 0.0837 \n",
      "Epoch 36/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9755 - loss: 0.0711 \n",
      "Epoch 37/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9781 - loss: 0.0801  \n",
      "Epoch 38/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - accuracy: 0.9768 - loss: 0.0717\n",
      "Epoch 39/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9681 - loss: 0.0739 \n",
      "Epoch 40/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0605 \n",
      "Epoch 41/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9803 - loss: 0.0597  \n",
      "Epoch 42/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9868 - loss: 0.0490  \n",
      "Epoch 43/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - accuracy: 0.9681 - loss: 0.0603\n",
      "Epoch 44/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9816 - loss: 0.0630 \n",
      "Epoch 45/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9781 - loss: 0.0584 \n",
      "Epoch 46/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9816 - loss: 0.0547 \n",
      "Epoch 47/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9794 - loss: 0.0610 \n",
      "Epoch 48/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9816 - loss: 0.0533 \n",
      "Epoch 49/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9807 - loss: 0.0434 \n",
      "Epoch 50/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9812 - loss: 0.0494 \n",
      "Epoch 51/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0457 \n",
      "Epoch 52/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9943 - loss: 0.0357 \n",
      "Epoch 53/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9816 - loss: 0.0446 \n",
      "Epoch 54/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9907 - loss: 0.0386 \n",
      "Epoch 55/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9817 - loss: 0.0511 \n",
      "Epoch 56/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9764 - loss: 0.0517  \n",
      "Epoch 57/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0470 \n",
      "Epoch 58/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9794 - loss: 0.0544 \n",
      "Epoch 59/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9738 - loss: 0.0547  \n",
      "Epoch 60/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9764 - loss: 0.0569 \n",
      "Epoch 61/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9799 - loss: 0.0414 \n",
      "Epoch 62/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9817 - loss: 0.0495 \n",
      "Epoch 63/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9807 - loss: 0.0423 \n",
      "Epoch 64/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203us/step - accuracy: 0.9886 - loss: 0.0499\n",
      "Epoch 65/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.9851 - loss: 0.0391\n",
      "Epoch 66/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0360 \n",
      "Epoch 67/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9925 - loss: 0.0364  \n",
      "Epoch 68/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9799 - loss: 0.0367  \n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - accuracy: 0.9834 - loss: 0.0493\n",
      "Epoch 70/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9912 - loss: 0.0407 \n",
      "Epoch 71/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9899 - loss: 0.0389 \n",
      "Epoch 72/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9807 - loss: 0.0491 \n",
      "Epoch 73/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414us/step - accuracy: 0.9755 - loss: 0.0417\n",
      "Epoch 74/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9847 - loss: 0.0335 \n",
      "Epoch 75/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9764 - loss: 0.0400  \n",
      "Epoch 76/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.9768 - loss: 0.0364\n",
      "Epoch 77/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0298 \n",
      "Epoch 78/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9781 - loss: 0.0362 \n",
      "Epoch 79/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9816 - loss: 0.0331 \n",
      "Epoch 80/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9895 - loss: 0.0353 \n",
      "Epoch 81/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9912 - loss: 0.0268 \n",
      "Epoch 82/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0458 \n",
      "Epoch 83/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0407 \n",
      "Epoch 84/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9820 - loss: 0.0380 \n",
      "Epoch 85/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9712 - loss: 0.0456 \n",
      "Epoch 86/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9930 - loss: 0.0285 \n",
      "Epoch 87/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0439 \n",
      "Epoch 88/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0326 \n",
      "Epoch 89/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9686 - loss: 0.0534 \n",
      "Epoch 90/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.0409 \n",
      "Epoch 91/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0420 \n",
      "Epoch 92/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0260 \n",
      "Epoch 93/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9781 - loss: 0.0358 \n",
      "Epoch 94/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9791 - loss: 0.0469 \n",
      "Epoch 95/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9912 - loss: 0.0243  \n",
      "Epoch 96/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0390 \n",
      "Epoch 97/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9834 - loss: 0.0324 \n",
      "Epoch 98/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0270 \n",
      "Epoch 99/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9943 - loss: 0.0243 \n",
      "Epoch 100/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9886 - loss: 0.0254 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21b61c8e980>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "model3.fit(x_values,y_values,epochs=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> node수를 늘리니 놀랍게도 정확도가 올랐습니다아~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증 데이터세트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 우리는 신경망이 여러 Epoch 동안 훈련될 수 있음을 알았습니다. 이는 네트워크가 동일한 데이터 세트로 여러 번 계속 학습할 수 있음을 의미합니다. 모델이 동일한 데이터 세트로 계속 학습하면 어떻게 될까요? 이러한 과정으로 네트워크의 정확도가 높아질 수 있을까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 정확도는 높아질 수 있겠지만, 숨 쉬듯이 과적합 문제가 오겠죠?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 계속 학습함에 따라 모델의 정확도가 증가할 것입니다. 동일한 데이터 세트로 시험해 볼 수 있습니다. 아래 코드를 실행하고 정확도를 관찰해 보세요. 동일한 설정으로 이전 모델보다 정확도가 높아졌습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.3155 - loss: 1.8295  \n",
      "Epoch 2/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.3685 - loss: 1.6661\n",
      "Epoch 3/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2982 - loss: 1.7349 \n",
      "Epoch 4/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3073 - loss: 1.6507 \n",
      "Epoch 5/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3815 - loss: 1.5305 \n",
      "Epoch 6/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2903 - loss: 1.5790 \n",
      "Epoch 7/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3641 - loss: 1.4709  \n",
      "Epoch 8/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - accuracy: 0.3211 - loss: 1.4721\n",
      "Epoch 9/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3098 - loss: 1.4413 \n",
      "Epoch 10/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3324 - loss: 1.4116 \n",
      "Epoch 11/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3215 - loss: 1.3712 \n",
      "Epoch 12/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3088 - loss: 1.3299  \n",
      "Epoch 13/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - accuracy: 0.3031 - loss: 1.3145\n",
      "Epoch 14/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2780 - loss: 1.3386 \n",
      "Epoch 15/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2840 - loss: 1.2957 \n",
      "Epoch 16/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2926 - loss: 1.2842 \n",
      "Epoch 17/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2830 - loss: 1.2620 \n",
      "Epoch 18/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3112 - loss: 1.2216 \n",
      "Epoch 19/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2595 - loss: 1.2306 \n",
      "Epoch 20/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1952 - loss: 1.2480 \n",
      "Epoch 21/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2433 - loss: 1.2069 \n",
      "Epoch 22/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2255 - loss: 1.1887 \n",
      "Epoch 23/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2268 - loss: 1.2009 \n",
      "Epoch 24/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2593 - loss: 1.1625 \n",
      "Epoch 25/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2753 - loss: 1.1498  \n",
      "Epoch 26/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - accuracy: 0.2810 - loss: 1.1280\n",
      "Epoch 27/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2362 - loss: 1.1564 \n",
      "Epoch 28/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2519 - loss: 1.1118 \n",
      "Epoch 29/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2302 - loss: 1.1338 \n",
      "Epoch 30/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2545 - loss: 1.1098 \n",
      "Epoch 31/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2619 - loss: 1.1116 \n",
      "Epoch 32/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2620 - loss: 1.1067 \n",
      "Epoch 33/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2802 - loss: 1.0980 \n",
      "Epoch 34/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2781 - loss: 1.1115 \n",
      "Epoch 35/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - accuracy: 0.2798 - loss: 1.0789\n",
      "Epoch 36/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2802 - loss: 1.0777 \n",
      "Epoch 37/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2638 - loss: 1.0895 \n",
      "Epoch 38/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2564 - loss: 1.0790 \n",
      "Epoch 39/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2894 - loss: 1.0563 \n",
      "Epoch 40/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2969 - loss: 1.0539 \n",
      "Epoch 41/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2736 - loss: 1.0671 \n",
      "Epoch 42/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3148 - loss: 1.0415 \n",
      "Epoch 43/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2939 - loss: 1.0476 \n",
      "Epoch 44/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440us/step - accuracy: 0.2987 - loss: 1.0572\n",
      "Epoch 45/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2880 - loss: 1.0388 \n",
      "Epoch 46/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3082 - loss: 1.0495 \n",
      "Epoch 47/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3481 - loss: 1.0172 \n",
      "Epoch 48/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3770 - loss: 1.0135 \n",
      "Epoch 49/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3637 - loss: 1.0236 \n",
      "Epoch 50/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4024 - loss: 1.0142 \n",
      "Epoch 51/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - accuracy: 0.4174 - loss: 1.0306\n",
      "Epoch 52/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4436 - loss: 0.9982 \n",
      "Epoch 53/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4230 - loss: 1.0161 \n",
      "Epoch 54/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4992 - loss: 1.0031 \n",
      "Epoch 55/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5405 - loss: 0.9870 \n",
      "Epoch 56/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5573 - loss: 0.9641 \n",
      "Epoch 57/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5566 - loss: 0.9800 \n",
      "Epoch 58/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.5831 - loss: 0.9544  \n",
      "Epoch 59/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.5815 - loss: 0.9731  \n",
      "Epoch 60/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5702 - loss: 0.9564 \n",
      "Epoch 61/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6032 - loss: 0.9633 \n",
      "Epoch 62/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6232 - loss: 0.9587 \n",
      "Epoch 63/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6102 - loss: 0.9344 \n",
      "Epoch 64/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.5972 - loss: 0.9441  \n",
      "Epoch 65/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416us/step - accuracy: 0.6285 - loss: 0.9182\n",
      "Epoch 66/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6024 - loss: 0.9240 \n",
      "Epoch 67/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6155 - loss: 0.8834 \n",
      "Epoch 68/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5465 - loss: 0.9020 \n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6315 - loss: 0.8668 \n",
      "Epoch 70/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6429 - loss: 0.8516 \n",
      "Epoch 71/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6399 - loss: 0.8386 \n",
      "Epoch 72/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6508 - loss: 0.8085 \n",
      "Epoch 73/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6062 - loss: 0.8196 \n",
      "Epoch 74/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.6606 - loss: 0.7966  \n",
      "Epoch 75/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6867 - loss: 0.7848 \n",
      "Epoch 76/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6861 - loss: 0.7783 \n",
      "Epoch 77/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.7520 - loss: 0.7404  \n",
      "Epoch 78/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7005 - loss: 0.7568 \n",
      "Epoch 79/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7061 - loss: 0.7214 \n",
      "Epoch 80/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6788 - loss: 0.7142 \n",
      "Epoch 81/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7183 - loss: 0.7102 \n",
      "Epoch 82/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7148 - loss: 0.7050 \n",
      "Epoch 83/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7470 - loss: 0.6813 \n",
      "Epoch 84/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207us/step - accuracy: 0.7397 - loss: 0.6612\n",
      "Epoch 85/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8057 - loss: 0.6210 \n",
      "Epoch 86/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.7858 - loss: 0.6213  \n",
      "Epoch 87/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.7536 - loss: 0.6151  \n",
      "Epoch 88/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - accuracy: 0.7615 - loss: 0.6028\n",
      "Epoch 89/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7454 - loss: 0.5831 \n",
      "Epoch 90/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.7393 - loss: 0.6082  \n",
      "Epoch 91/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.7537 - loss: 0.5878  \n",
      "Epoch 92/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - accuracy: 0.7697 - loss: 0.5648\n",
      "Epoch 93/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7661 - loss: 0.5549 \n",
      "Epoch 94/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7400 - loss: 0.5388 \n",
      "Epoch 95/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7684 - loss: 0.5481 \n",
      "Epoch 96/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.7637 - loss: 0.5363  \n",
      "Epoch 97/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step - accuracy: 0.7841 - loss: 0.5153\n",
      "Epoch 98/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6886 - loss: 0.5710 \n",
      "Epoch 99/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7833 - loss: 0.4937 \n",
      "Epoch 100/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7677 - loss: 0.4968 \n",
      "Epoch 101/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7881 - loss: 0.4922 \n",
      "Epoch 102/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7473 - loss: 0.5053 \n",
      "Epoch 103/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.7321 - loss: 0.4975  \n",
      "Epoch 104/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - accuracy: 0.7664 - loss: 0.4774\n",
      "Epoch 105/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7700 - loss: 0.4749 \n",
      "Epoch 106/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8112 - loss: 0.4688 \n",
      "Epoch 107/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7826 - loss: 0.4501 \n",
      "Epoch 108/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7730 - loss: 0.4675 \n",
      "Epoch 109/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7622 - loss: 0.4600 \n",
      "Epoch 110/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.7774 - loss: 0.4534  \n",
      "Epoch 111/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7991 - loss: 0.4532 \n",
      "Epoch 112/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7804 - loss: 0.4334 \n",
      "Epoch 113/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7939 - loss: 0.4264 \n",
      "Epoch 114/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7883 - loss: 0.4550 \n",
      "Epoch 115/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8118 - loss: 0.4166 \n",
      "Epoch 116/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.7975 - loss: 0.4251  \n",
      "Epoch 117/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.8066 - loss: 0.4181  \n",
      "Epoch 118/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - accuracy: 0.7641 - loss: 0.4286\n",
      "Epoch 119/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8001 - loss: 0.3977 \n",
      "Epoch 120/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8225 - loss: 0.4177 \n",
      "Epoch 121/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8668 - loss: 0.3885 \n",
      "Epoch 122/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8568 - loss: 0.3701 \n",
      "Epoch 123/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8738 - loss: 0.3660 \n",
      "Epoch 124/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8452 - loss: 0.3948 \n",
      "Epoch 125/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8539 - loss: 0.3727 \n",
      "Epoch 126/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8869 - loss: 0.3640 \n",
      "Epoch 127/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8678 - loss: 0.3618 \n",
      "Epoch 128/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8457 - loss: 0.3793 \n",
      "Epoch 129/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8587 - loss: 0.3617 \n",
      "Epoch 130/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8461 - loss: 0.3599 \n",
      "Epoch 131/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8761 - loss: 0.3373 \n",
      "Epoch 132/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9030 - loss: 0.3236 \n",
      "Epoch 133/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8397 - loss: 0.3524 \n",
      "Epoch 134/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8562 - loss: 0.3296 \n",
      "Epoch 135/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8588 - loss: 0.3268 \n",
      "Epoch 136/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9018 - loss: 0.3095 \n",
      "Epoch 137/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8819 - loss: 0.3359 \n",
      "Epoch 138/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9002 - loss: 0.3107 \n",
      "Epoch 139/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8806 - loss: 0.3351 \n",
      "Epoch 140/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8871 - loss: 0.3136 \n",
      "Epoch 141/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8958 - loss: 0.3145 \n",
      "Epoch 142/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9080 - loss: 0.2840 \n",
      "Epoch 143/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8841 - loss: 0.3010 \n",
      "Epoch 144/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8789 - loss: 0.3041 \n",
      "Epoch 145/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8824 - loss: 0.2920 \n",
      "Epoch 146/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8889 - loss: 0.3045 \n",
      "Epoch 147/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9067 - loss: 0.2643 \n",
      "Epoch 148/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9019 - loss: 0.2766  \n",
      "Epoch 149/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8907 - loss: 0.2770 \n",
      "Epoch 150/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8798 - loss: 0.2799 \n",
      "Epoch 151/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9041 - loss: 0.2714 \n",
      "Epoch 152/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9011 - loss: 0.2508 \n",
      "Epoch 153/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9020 - loss: 0.2632 \n",
      "Epoch 154/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9025 - loss: 0.2835 \n",
      "Epoch 155/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9172 - loss: 0.2481 \n",
      "Epoch 156/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9138 - loss: 0.2590  \n",
      "Epoch 157/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8903 - loss: 0.2991 \n",
      "Epoch 158/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9230 - loss: 0.2512 \n",
      "Epoch 159/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9256 - loss: 0.2376 \n",
      "Epoch 160/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9191 - loss: 0.2552  \n",
      "Epoch 161/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - accuracy: 0.8878 - loss: 0.2743\n",
      "Epoch 162/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9039 - loss: 0.2396 \n",
      "Epoch 163/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9082 - loss: 0.2473 \n",
      "Epoch 164/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9152 - loss: 0.2514 \n",
      "Epoch 165/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9361 - loss: 0.2216 \n",
      "Epoch 166/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9492 - loss: 0.2211 \n",
      "Epoch 167/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9457 - loss: 0.2126  \n",
      "Epoch 168/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269us/step - accuracy: 0.9544 - loss: 0.2271\n",
      "Epoch 169/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9406 - loss: 0.2192 \n",
      "Epoch 170/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9410 - loss: 0.2145 \n",
      "Epoch 171/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9531 - loss: 0.2009 \n",
      "Epoch 172/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9544 - loss: 0.2081 \n",
      "Epoch 173/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9380 - loss: 0.2178 \n",
      "Epoch 174/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9579 - loss: 0.1847 \n",
      "Epoch 175/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9479 - loss: 0.2152 \n",
      "Epoch 176/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9570 - loss: 0.2013 \n",
      "Epoch 177/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9553 - loss: 0.1942 \n",
      "Epoch 178/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - accuracy: 0.9449 - loss: 0.2064\n",
      "Epoch 179/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9410 - loss: 0.1868 \n",
      "Epoch 180/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9458 - loss: 0.2050 \n",
      "Epoch 181/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9640 - loss: 0.1891 \n",
      "Epoch 182/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9544 - loss: 0.1884  \n",
      "Epoch 183/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9393 - loss: 0.1898 \n",
      "Epoch 184/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9471 - loss: 0.1969 \n",
      "Epoch 185/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9492 - loss: 0.1728 \n",
      "Epoch 186/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9267 - loss: 0.1995 \n",
      "Epoch 187/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9423 - loss: 0.1924 \n",
      "Epoch 188/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9518 - loss: 0.1888 \n",
      "Epoch 189/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9488 - loss: 0.1838  \n",
      "Epoch 190/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9267 - loss: 0.1843 \n",
      "Epoch 191/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9636 - loss: 0.1585 \n",
      "Epoch 192/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9518 - loss: 0.1758 \n",
      "Epoch 193/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9215 - loss: 0.1852 \n",
      "Epoch 194/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9471 - loss: 0.1767 \n",
      "Epoch 195/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9432 - loss: 0.1552 \n",
      "Epoch 196/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9588 - loss: 0.1604 \n",
      "Epoch 197/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9294 - loss: 0.1871 \n",
      "Epoch 198/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9619 - loss: 0.1569 \n",
      "Epoch 199/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9511 - loss: 0.1673 \n",
      "Epoch 200/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.9702 - loss: 0.1468  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21b62eb0880>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 신경망 모델을 초기화\n",
    "model4 = Sequential()\n",
    "\n",
    "# 6개의 노드가 있는 첫 번째 은닉 레이어를 추가합니다.\n",
    "# Input_dim은 x_values 또는 입력 레이어의 수/특성 수를 나타냅니다.\n",
    "# activation은 노드/뉴런이 활성화되는 방식을 나타냅니다. 우리는 relu를 사용할 것입니다. 다른 일반적인 활성화 방식은'sigmoid' 및 'tanh'입니다.\n",
    "model4.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# 6개의 노드가 있는 두 번째 은닉 레이어를 추가합니다. \n",
    "model4.add(Dense(6,activation='relu'))\n",
    "\n",
    "# 3개의 노드가 있는 출력 레이어를 추가합니다.\n",
    "# 사용된 activation은 'softmax'입니다. Softmax는 범주형 출력 또는 대상을 처리할 때 사용됩니다.\n",
    "model4.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# 모델을 컴파일합니다. optimizer는 모델 내에서 조정하는 방법을 의미합니다. loss은 예측된 출력과 실제 출력 간의 차이를 나타냅니다.\n",
    "model4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# x_values와 y_values로 모델을 훈련시킵니다.\n",
    "# Epoch는 전체 데이터 세트가 모델을 학습하는 데 사용되는 횟수를 나타냅니다.\n",
    "# Shuffle = True는 모델이 각 Epoch 후에 데이터 세트의 배열을 무작위로 지정하도록 지시합니다.\n",
    "model4.fit(x_values,y_values,epochs=200,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도 점수가 이제 90% 이상임을 출력에서 확인할 수 있습니다. 에포크 수를 늘리는 것만으로도 정확도를 높일 수 있습니다. 모델이 훈련된 데이터에 대해서만 정확하다는 것이 좋다고 생각하십니까? \n",
    "축구 선수가 경기장의 특정 지점에서 골을 넣는 것만 연습한다면 경기에서도 좋은 결과를 얻을 수 있을까요? 제빵사가 특정한 맛, 모양, 크기의 케이크를 굽는 방법만 배운다면 고객의 요청에 따라 맛있는 케이크를 만들 수 있을까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축구 선수는 운동장의 다른 지역에서는 정확하게 슛을 쏘지 못할 수 있으므로 경기에서 좋은 결과를 얻기 어려울 수 있습니다. 제빵사는 한 가지 특정 맛만 알기 때문에 손님이 원하는 맛있는 케이크를 구울 수 없습니다.\n",
    "\n",
    "이 질문의 이면에 있는 아이디어는 과적합에 대한 개념입니다. 모델이 과적합되면 새로운 데이터에 대하여 예측 성능이나 정확도가 떨어질 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것은 과적합의 개념이며 모든 기계 학습 기술에도 적용됩니다. 모델이 데이터 세트에 너무 정확하게 맞춰지면 훈련된 모델은 이전에 본 적이 없는 새로운 데이터에 대하여 일반화하여 사용할 수 없습니다. 따라서 우리는 일반적으로 보유하고 있는 데이터 세트의 일부에 대해서만 모델을 훈련하고 나머지는 모델이 과적합되었는지 확인하기 위해 테스트 또는 검증 데이터 세트로 유지해야 합니다. Epoch가 증가함에 따라 훈련 세트로 학습시키 모델의 테스트 세트의 정확도는 높아져야 합니다. 그러나 과적합 지점에서 테스트 세트의 정확도가 감소하기 시작합니다. 특정 Epoch 이후에 테스트 정확도가 증가하기 시작했다면 더 이상 모델을 훈련해서는 안 됩니다.\n",
    "\n",
    "여기에서 사용하는 데이터 세트를 훈련 세트와 테스트 세트로 나누어 적용해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, 모델이 학습에 노출되지 않고 유지할 데이터의 양을 결정해야 합니다. 일반적으로 전체 데이터의 20~30%를 테스트 세트로 유지합니다. 이 예에서는 데이터 세트의 25%를 테스트/검증 세트로 유지합니다. sklearn 라이브러리에서 train_test_split 함수를 합니다. 아래 코드를 실행해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in x_train: 112\n",
      "Number of rows in x_test: 38\n",
      "Number of rows in y_train: 112\n",
      "Number of rows in y_test: 38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 프레임 df에서 원래 x_values를 추출합니다.\n",
    "# 표준화는 모델이 학습할 데이터에만 기반해야 하므로 x_values를 다시 추출해야 합니다.\n",
    "# 따라서 표준화하기 전에 먼저 데이터를 분할해야 합니다.\n",
    "x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "\n",
    "# Test_size=0.25는 전체 데이터의 25%가 x_test 및 y_test로 배정되면75%는 x_train 및 y_train에 배정됨을 나타냅니다.\n",
    "# random_state=10은 아래 코드를 실행할 때마다 분할이 동일하도록 하는 데 사용됩니다.\n",
    "# 분할은 매번 랜덤으로 하기 때문입니다. 동일한 random_state는 매번 동일하게 분할되도록 보장하는 유일한 방법입니다.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_values,y_values,test_size=0.25,random_state=10)\n",
    "\n",
    "# x_train, x_test, y_train 및 y_test의 행 수 확인\n",
    "print(\"Number of rows in x_train:\", x_train.shape[0])\n",
    "print(\"Number of rows in x_test:\", x_test.shape[0])\n",
    "print(\"Number of rows in y_train:\", y_train.shape[0])\n",
    "print(\"Number of rows in y_test:\", y_test.shape[0])\n",
    "\n",
    "# 이제 x 값을 표준화할 수 있습니다.\n",
    "# StandardScaler 초기화\n",
    "standardise = StandardScaler()\n",
    "\n",
    "# .fit_transform을 사용하여 x_train 값을 표준화합니다.\n",
    "x_train = standardise.fit_transform(x_train)\n",
    "\n",
    "# .transform을 사용하여 x_test 값을 표준화합니다.\n",
    "# 표준화는 x_train과 같아야 하므로 데이터를 맞출 필요가 없습니다.\n",
    "x_test = standardise.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 세트의 20%를 테스트/검증 세트로 유지하여 데이터 세트를 훈련 및 테스트/검증 세트로 분할하는 코드를 작성하십시오. \n",
    "x_train2, x_test2, y_train2 및 y_test2를 변수로 사용하십시오. 각 변수에 대한 행 수가 올바른지 확인하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer here\n",
    "x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_values,y_values,test_size=0.25,random_state=10)\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x_train,y_train,test_size=0.2,random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 x_train2, x_test2, y_train2 및 y_test2를 사용하여 신경망을 훈련시킵니다. 각각 6개의 노드가 있는 2개의 은닉 레이어와 3개의 노드가 있는 1개의 출력 레이어가 있는 신경망을 만드는 코드를 작성하십시오. 처음 은닉 레이어의 활성화(activation)는 'relu'인 반면 출력 레이어에 대한 활성화는 'softmax'입니다. 사용할 옵티마이저(optimizer)는 'adam'이고 손실(loss)은 'categorical_crossentropy'여야 합니다. 측정 항목(metrics)는 '정확도(accuracy)'입니다. model_val을 모델 변수로 사용하십시오. 모델을 컴파일한 후 모델 요약을 출력하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IOT\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m4,128\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m330\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,131</span> (20.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,131\u001b[0m (20.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,131</span> (20.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,131\u001b[0m (20.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code/answer here\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(128,input_dim=4,activation='relu'))\n",
    "model4.add(Dense(64,activation='softmax'))\n",
    "model4.add(Dense(10,activation='softmax'))\n",
    "model4.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# 모델을 컴파일합니다. optimizer는 모델 내에서 조정하는 방법을 의미합니다. loss은 예측된 출력과 실제 출력 간의 차이를 나타냅니다.\n",
    "model4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# 모델 요약 출력\n",
    "model4.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 x_train 및 y_train을 사용하여 model_val을 훈련할 수 있습니다. 또한 x_test 및 y_test를 사용하여 각 에포크 후에 정확도를 테스트합니다. 아래 코드를 실행해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.3673 - loss: 1.0975 - val_accuracy: 0.2609 - val_loss: 1.1083\n",
      "Epoch 2/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3438 - loss: 1.0983 - val_accuracy: 0.2609 - val_loss: 1.1078\n",
      "Epoch 3/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3751 - loss: 1.0933 - val_accuracy: 0.2609 - val_loss: 1.1076\n",
      "Epoch 4/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3673 - loss: 1.0938 - val_accuracy: 0.2609 - val_loss: 1.1073\n",
      "Epoch 5/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3829 - loss: 1.0886 - val_accuracy: 0.2609 - val_loss: 1.1071\n",
      "Epoch 6/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3595 - loss: 1.0888 - val_accuracy: 0.2609 - val_loss: 1.1065\n",
      "Epoch 7/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3946 - loss: 1.0870 - val_accuracy: 0.2609 - val_loss: 1.1058\n",
      "Epoch 8/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3360 - loss: 1.0875 - val_accuracy: 0.2609 - val_loss: 1.1046\n",
      "Epoch 9/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3438 - loss: 1.0897 - val_accuracy: 0.2609 - val_loss: 1.1031\n",
      "Epoch 10/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3829 - loss: 1.0843 - val_accuracy: 0.2609 - val_loss: 1.1014\n",
      "Epoch 11/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3907 - loss: 1.0784 - val_accuracy: 0.2609 - val_loss: 1.0993\n",
      "Epoch 12/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3634 - loss: 1.0802 - val_accuracy: 0.2609 - val_loss: 1.0969\n",
      "Epoch 13/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3573 - loss: 1.0825 - val_accuracy: 0.2609 - val_loss: 1.0946\n",
      "Epoch 14/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4222 - loss: 1.0740 - val_accuracy: 0.4783 - val_loss: 1.0924\n",
      "Epoch 15/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6288 - loss: 1.0753 - val_accuracy: 0.6087 - val_loss: 1.0903\n",
      "Epoch 16/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6764 - loss: 1.0758 - val_accuracy: 0.6087 - val_loss: 1.0883\n",
      "Epoch 17/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7211 - loss: 1.0682 - val_accuracy: 0.6087 - val_loss: 1.0866\n",
      "Epoch 18/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6860 - loss: 1.0724 - val_accuracy: 0.6087 - val_loss: 1.0848\n",
      "Epoch 19/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7485 - loss: 1.0597 - val_accuracy: 0.6087 - val_loss: 1.0833\n",
      "Epoch 20/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6899 - loss: 1.0677 - val_accuracy: 0.6087 - val_loss: 1.0817\n",
      "Epoch 21/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7602 - loss: 1.0535 - val_accuracy: 0.6087 - val_loss: 1.0804\n",
      "Epoch 22/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7016 - loss: 1.0616 - val_accuracy: 0.6087 - val_loss: 1.0789\n",
      "Epoch 23/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7133 - loss: 1.0574 - val_accuracy: 0.6087 - val_loss: 1.0776\n",
      "Epoch 24/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7094 - loss: 1.0582 - val_accuracy: 0.6087 - val_loss: 1.0762\n",
      "Epoch 25/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7172 - loss: 1.0551 - val_accuracy: 0.6087 - val_loss: 1.0749\n",
      "Epoch 26/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7094 - loss: 1.0547 - val_accuracy: 0.6087 - val_loss: 1.0735\n",
      "Epoch 27/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7328 - loss: 1.0494 - val_accuracy: 0.6087 - val_loss: 1.0723\n",
      "Epoch 28/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7328 - loss: 1.0478 - val_accuracy: 0.6087 - val_loss: 1.0710\n",
      "Epoch 29/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6977 - loss: 1.0529 - val_accuracy: 0.6087 - val_loss: 1.0697\n",
      "Epoch 30/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7055 - loss: 1.0491 - val_accuracy: 0.6087 - val_loss: 1.0684\n",
      "Epoch 31/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7289 - loss: 1.0440 - val_accuracy: 0.6087 - val_loss: 1.0671\n",
      "Epoch 32/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6703 - loss: 1.0534 - val_accuracy: 0.6087 - val_loss: 1.0658\n",
      "Epoch 33/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7133 - loss: 1.0439 - val_accuracy: 0.6087 - val_loss: 1.0645\n",
      "Epoch 34/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7055 - loss: 1.0446 - val_accuracy: 0.6087 - val_loss: 1.0631\n",
      "Epoch 35/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7172 - loss: 1.0402 - val_accuracy: 0.6087 - val_loss: 1.0618\n",
      "Epoch 36/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6899 - loss: 1.0442 - val_accuracy: 0.6087 - val_loss: 1.0604\n",
      "Epoch 37/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6703 - loss: 1.0466 - val_accuracy: 0.6087 - val_loss: 1.0589\n",
      "Epoch 38/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7446 - loss: 1.0299 - val_accuracy: 0.6087 - val_loss: 1.0575\n",
      "Epoch 39/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7016 - loss: 1.0370 - val_accuracy: 0.6087 - val_loss: 1.0559\n",
      "Epoch 40/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6860 - loss: 1.0386 - val_accuracy: 0.6087 - val_loss: 1.0540\n",
      "Epoch 41/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6782 - loss: 1.0384 - val_accuracy: 0.6087 - val_loss: 1.0522\n",
      "Epoch 42/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7094 - loss: 1.0296 - val_accuracy: 0.6087 - val_loss: 1.0503\n",
      "Epoch 43/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7211 - loss: 1.0249 - val_accuracy: 0.6087 - val_loss: 1.0483\n",
      "Epoch 44/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7250 - loss: 1.0219 - val_accuracy: 0.6087 - val_loss: 1.0464\n",
      "Epoch 45/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7211 - loss: 1.0206 - val_accuracy: 0.6087 - val_loss: 1.0443\n",
      "Epoch 46/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7211 - loss: 1.0187 - val_accuracy: 0.6087 - val_loss: 1.0423\n",
      "Epoch 47/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7328 - loss: 1.0135 - val_accuracy: 0.6087 - val_loss: 1.0402\n",
      "Epoch 48/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6703 - loss: 1.0258 - val_accuracy: 0.6087 - val_loss: 1.0381\n",
      "Epoch 49/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7172 - loss: 1.0131 - val_accuracy: 0.6087 - val_loss: 1.0361\n",
      "Epoch 50/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7328 - loss: 1.0069 - val_accuracy: 0.6087 - val_loss: 1.0342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21b64183d60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_values와 y_values로 모델을 훈련시킵니다.\n",
    "# Epoch는 전체 데이터 세트가 모델을 학습하는 데 사용되는 횟수를 나타냅니다.\n",
    "# Shuffle = True는 모델이 각 Epoch 후에 데이터 세트의 배열을 무작위로 지정하도록 지시합니다. 이렇게 하면 모델이 학습할 수 있습니다.\n",
    "# Validation_data를 사용하면 테스트/검증 데이터 세트를 입력할 수 있습니다. 이를 통해 테스트/검증 세트에서 모델 정확도를 확인할 수 있습니다.\n",
    "\n",
    "model4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model4.fit(x_train2,y_train2,epochs=50,shuffle=True, validation_data=(x_test2,y_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력에 검증 데이터의 정확도도 같이 표시되고 있습니다. 검증 정확도가 훈련 정확도보다 높은가요?아니면 낮은가요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 낮습니다잉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 모델을 사용하여 새로 수집된 데이터의 꽃 유형을 식별할 수 있습니다. 예를 들어, 친구가 일부 꽃의 꽃받침 길이, 꽃받침 너비, 꽃잎 길이 및 꽃잎 너비를 측정하고 \"iris_predict.data\"라는 파일에 데이터를 저장했다고 가정해 봅시다. 친구가 측정된 값을 기반으로 이 꽃의 종류를 찾고 싶어합니다. 친구를 돕기 위해 분류 모델을 사용하여 친구가 찾은 꽃의 종류를 찾아보세요. 아래 코드를 실행해 보세요!\n",
    "<font color=blue>힌트: model.predict 메서드를 사용하여 친구의 꽃 종류를 얻을 수 있습니다. 또한 .predict 메서드에서 반환된 값은 모델이 각 데이터 행에 할당해야 한다고 생각하는 각 꽃 유형의 확률입니다. 따라서 확률이 높을수록 모델이 예측한 꽃 유형이 정확하다는 확신을 가질 수 있습니다. 예를 들어 예측 값이 두 번째 열에서 매우 높으면 모델은 꽃 유형이 versicolor라고 생각할 수 있습니다. 또한 꽃 유형을 얻기 전에 데이터를 스케일링해야 합니다. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df = pd.read_csv(\"./data/iris.data\",header=None)\n",
    "names = [\"sepal_length\", \"sepal_width\",\"petal_length\", \"petal_width\",\"class\"]\n",
    "df2.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = df2[['sepal_length','sepal_width','petal_length','petal_width']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측할 data도 scaling이 필요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new_scale = standardise.transform(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n"
     ]
    }
   ],
   "source": [
    "y_new = model.predict(x_new_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5023041  0.1949924  0.30270344]\n",
      " [0.5115222  0.17551363 0.31296414]\n",
      " [0.5054275  0.18106537 0.3135071 ]\n",
      " [0.5038286  0.17833476 0.3178367 ]\n",
      " [0.49850193 0.19836059 0.30313748]\n",
      " [0.45782188 0.27093068 0.27124748]\n",
      " [0.48414567 0.2049762  0.31087822]\n",
      " [0.502166   0.19139461 0.30643943]\n",
      " [0.44065192 0.16296935 0.3963787 ]\n",
      " [0.5211768  0.16502893 0.31379429]\n",
      " [0.4991696  0.21200489 0.2888255 ]\n",
      " [0.49822658 0.19110781 0.31066564]\n",
      " [0.52317715 0.16060711 0.31621578]\n",
      " [0.49530715 0.15173994 0.35295293]\n",
      " [0.49322104 0.2571033  0.24967565]\n",
      " [0.43406177 0.3407156  0.22522274]\n",
      " [0.4626129  0.2695532  0.26783392]\n",
      " [0.48799428 0.21174248 0.30026326]\n",
      " [0.47892118 0.2553625  0.26571637]\n",
      " [0.47864008 0.22561897 0.2957409 ]\n",
      " [0.50488424 0.1948573  0.30025843]\n",
      " [0.46635723 0.2391393  0.29450348]\n",
      " [0.49835545 0.19337259 0.308272  ]\n",
      " [0.45973438 0.24175152 0.29851404]\n",
      " [0.49440366 0.19327189 0.31232443]\n",
      " [0.51037383 0.17734265 0.3122835 ]\n",
      " [0.4721144  0.2262349  0.30165073]\n",
      " [0.502334   0.19623515 0.30143082]\n",
      " [0.5060914  0.19166477 0.30224383]\n",
      " [0.5016265  0.18314394 0.31522954]\n",
      " [0.5053187  0.17998646 0.31469485]\n",
      " [0.47829935 0.2277294  0.29397133]\n",
      " [0.49969035 0.21912782 0.2811818 ]\n",
      " [0.48204467 0.26231787 0.25563744]\n",
      " [0.5211768  0.16502893 0.31379429]\n",
      " [0.5107797  0.18181473 0.30740556]\n",
      " [0.50860304 0.1991052  0.29229173]\n",
      " [0.5211768  0.16502893 0.31379429]\n",
      " [0.47583342 0.1697978  0.35436875]\n",
      " [0.5034921  0.19189475 0.3046131 ]\n",
      " [0.48801517 0.21042308 0.3015618 ]\n",
      " [0.37317494 0.2205458  0.4062793 ]\n",
      " [0.50130516 0.17960343 0.31909144]\n",
      " [0.43859324 0.26914397 0.29226276]\n",
      " [0.4583178  0.24706168 0.2946205 ]\n",
      " [0.4966156  0.19046763 0.3129168 ]\n",
      " [0.49212256 0.20834906 0.29952836]\n",
      " [0.50279194 0.18126833 0.31593975]\n",
      " [0.49829635 0.2075992  0.29410443]\n",
      " [0.5058589  0.18691364 0.3072275 ]\n",
      " [0.28140303 0.43580192 0.28279513]\n",
      " [0.25515434 0.34793383 0.39691186]\n",
      " [0.279337   0.3960811  0.32458186]\n",
      " [0.13081644 0.5300938  0.3390898 ]\n",
      " [0.25518328 0.3640902  0.38072646]\n",
      " [0.17626862 0.4096945  0.4140369 ]\n",
      " [0.182795   0.2789857  0.5382193 ]\n",
      " [0.15592639 0.5065424  0.3375313 ]\n",
      " [0.31393558 0.40660983 0.27945465]\n",
      " [0.12662855 0.45239067 0.42098072]\n",
      " [0.11374417 0.53577095 0.35048482]\n",
      " [0.198067   0.32559994 0.476333  ]\n",
      " [0.22075252 0.49251384 0.28673357]\n",
      " [0.20587347 0.33442956 0.45969695]\n",
      " [0.23816623 0.47878855 0.28304523]\n",
      " [0.2951911  0.4244035  0.2804055 ]\n",
      " [0.1216921  0.2624535  0.6158544 ]\n",
      " [0.27046636 0.44481426 0.28471944]\n",
      " [0.15916605 0.44503868 0.39579523]\n",
      " [0.19431114 0.5230285  0.28266037]\n",
      " [0.07567526 0.13756889 0.7867558 ]\n",
      " [0.2844172  0.42950645 0.2860764 ]\n",
      " [0.17697583 0.36575216 0.45727205]\n",
      " [0.25415576 0.43295863 0.31288567]\n",
      " [0.32183063 0.39935505 0.27881435]\n",
      " [0.30411947 0.4102365  0.28564408]\n",
      " [0.3090573  0.4051808  0.2857619 ]\n",
      " [0.17945112 0.26958948 0.55095935]\n",
      " [0.18069667 0.31364882 0.5056545 ]\n",
      " [0.27335373 0.43956566 0.28708056]\n",
      " [0.1738483  0.5358972  0.29025453]\n",
      " [0.2061693  0.49567696 0.2981537 ]\n",
      " [0.23325282 0.48483318 0.28191397]\n",
      " [0.09953937 0.2288426  0.67161804]\n",
      " [0.0975733  0.24626431 0.6561624 ]\n",
      " [0.16880535 0.27456346 0.5566312 ]\n",
      " [0.28062743 0.38772446 0.33164808]\n",
      " [0.21177435 0.4727075  0.3155181 ]\n",
      " [0.20694952 0.40765917 0.38539132]\n",
      " [0.15350367 0.5171504  0.32934597]\n",
      " [0.16098215 0.5172262  0.32179156]\n",
      " [0.2215491  0.32612094 0.4523299 ]\n",
      " [0.21409258 0.5042778  0.28162953]\n",
      " [0.15378726 0.5117209  0.33449185]\n",
      " [0.17889905 0.47515276 0.34594822]\n",
      " [0.24062964 0.43702766 0.3223427 ]\n",
      " [0.20910063 0.42353666 0.36736268]\n",
      " [0.30065164 0.4100351  0.2893133 ]\n",
      " [0.1796394  0.52091485 0.2994457 ]\n",
      " [0.21079525 0.4628264  0.32637832]\n",
      " [0.00693772 0.01918627 0.97387606]\n",
      " [0.03965593 0.11321031 0.84713376]\n",
      " [0.06226284 0.11831611 0.8194211 ]\n",
      " [0.06331607 0.1184646  0.81821924]\n",
      " [0.02492791 0.05528651 0.91978556]\n",
      " [0.06295769 0.12040589 0.8166364 ]\n",
      " [0.03437833 0.21534276 0.75027895]\n",
      " [0.11132177 0.18859729 0.70008093]\n",
      " [0.0861578  0.16108055 0.75276166]\n",
      " [0.02421118 0.05912741 0.91666144]\n",
      " [0.07468882 0.13701126 0.7883    ]\n",
      " [0.07192883 0.13499877 0.79307234]\n",
      " [0.06082907 0.1155168  0.8236542 ]\n",
      " [0.0298447  0.10936522 0.86079013]\n",
      " [0.01054052 0.0311569  0.95830256]\n",
      " [0.02645447 0.05837419 0.9151714 ]\n",
      " [0.08602104 0.15257578 0.7614032 ]\n",
      " [0.07738028 0.16938093 0.7532388 ]\n",
      " [0.03390678 0.07129546 0.89479774]\n",
      " [0.11570818 0.41141215 0.4728797 ]\n",
      " [0.03512294 0.0743458  0.8905313 ]\n",
      " [0.02825277 0.0854946  0.88625264]\n",
      " [0.08248229 0.14872506 0.76879257]\n",
      " [0.10891744 0.20261061 0.688472  ]\n",
      " [0.04508504 0.09163888 0.8632761 ]\n",
      " [0.12621714 0.21094123 0.6628416 ]\n",
      " [0.10530326 0.19134031 0.7033565 ]\n",
      " [0.08801849 0.1543295  0.75765204]\n",
      " [0.0342168  0.07113673 0.89464647]\n",
      " [0.21174696 0.30973732 0.4785157 ]\n",
      " [0.1162475  0.19416875 0.6895837 ]\n",
      " [0.15274662 0.2960256  0.5512278 ]\n",
      " [0.0264091  0.05736687 0.916224  ]\n",
      " [0.16732866 0.27645612 0.5562152 ]\n",
      " [0.11341663 0.27315703 0.6134264 ]\n",
      " [0.06670395 0.12587926 0.80741674]\n",
      " [0.01381164 0.0347623  0.951426  ]\n",
      " [0.07668457 0.13960727 0.7837081 ]\n",
      " [0.08411272 0.15358882 0.76229846]\n",
      " [0.07396664 0.13602062 0.79001266]\n",
      " [0.02294967 0.05184837 0.92520195]\n",
      " [0.05918111 0.1132447  0.82757425]\n",
      " [0.03965593 0.11321031 0.84713376]\n",
      " [0.02576991 0.05752292 0.9167072 ]\n",
      " [0.01592914 0.03858794 0.9454829 ]\n",
      " [0.04267406 0.08612561 0.87120026]\n",
      " [0.08082637 0.18433742 0.7348362 ]\n",
      " [0.06927918 0.12786429 0.80285656]\n",
      " [0.019426   0.04651184 0.93406206]\n",
      " [0.05775895 0.11964639 0.82259464]]\n"
     ]
    }
   ],
   "source": [
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 데이터 행의 꽃 유형은 무엇입니까? [argmax](https://www.geeksforgeeks.org/numpy-argmax-python/) 함수를 사용하여 y_new에서 꽃 종류를 식별할 수 있습니다. 아래 코드를 실행해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "flower_types = []\n",
    "for ii in range(0,y_new.shape[0]):\n",
    "    # np.argmax를 쓰는 이유는 입력된 test data에 \n",
    "    # 3개의 label의 각각 예측 값이 있어서 개중에 제일 큰 값을 사용 해야함\n",
    "    flower_types.append(np.argmax(y_new[ii,:]))\n",
    "print(flower_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번호로 표기된 꽃의 유형을 다시 원래의 클래스 정보(Setosa, Versicolor, Virginica)로 변경할 수 있습니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Virginica',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Setosa',\n",
       " 'Versicolor',\n",
       " 'Virginica',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Virginica',\n",
       " 'Versicolor',\n",
       " 'Virginica',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Virginica',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Virginica',\n",
       " 'Versicolor',\n",
       " 'Virginica',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Virginica',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Versicolor',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica',\n",
       " 'Virginica']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your answer here\n",
    "flower_name = {0: \"Setosa\", 1: \"Versicolor\", 2: \"Virginica\"}\n",
    "converted_list = [flower_name[flower] for flower in flower_types]\n",
    "converted_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축하합니다! 인공 신경망 모델을 만들고 훈련하는 방법을 성공적으로 배웠습니다. 인공 신경망은 매우 큰 데이터 세트에 사용할 수 있는 매우 강력한 도구입니다. 예를 들어, 수백 개의 열/특성과 100,000개 이상의 데이터 포인트가 있는 경우 다른 기계 학습 기술 대신 인공 신경망을 사용하는 것이 유리할 수 있습니다. 은닉 레이어 노드 수 또는 은닉 레이어 수에 대한 엄격한 규칙은 없습니다. 다양한 신경망을 실험하여 데이터 세트에 가장 적합한 결과를 제공하는 신경망을 찾는 것이 중요합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
